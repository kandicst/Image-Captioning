{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "from data_preprocessing import *\n",
    "import urllib.request\n",
    "import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data_train_dir = 'extracted_data/train/'\n",
    "extracted_data_test_dir = 'extracted_data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    with open(file, 'r') as f1:\n",
    "        return json.loads(f1.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = load_json(extracted_data_train_dir + 'captions.json')\n",
    "train_image_paths = load_json(extracted_data_train_dir + 'image_paths.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> A bathroom with a TV near the mirror <END>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mscoco/train/img/COCO_train2014_000000028149'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_paths[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_captions = [x[8:] for x in train_captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mscoco/train/img/COCO_train2014_000000318556 <START> A very clean and well decorated empty bathroom <END>\n"
     ]
    }
   ],
   "source": [
    "print(train_image_paths[0], train_captions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = Vocabulary(train_captions, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfdx = Vocabulary(train_captions, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 145, 430, 10, 667, 450, 237, 35, 2]\n",
      "<start> a very clean and well decorated empty bathroom <end>\n",
      "<START> A very clean and well decorated empty bathroom <END>\n"
     ]
    }
   ],
   "source": [
    "print(train_vocab.encoded_captions[0])\n",
    "print(train_vocab.decode_sentence(train_vocab.encoded_captions[0]))\n",
    "print(train_captions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create encoded captions for each caption in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 145, 430, 10, 667, 450, 237, 35, 2]]\n",
      "<start> a very clean and well decorated empty bathroom <end>\n",
      "['<START> A very clean and well decorated empty bathroom <END>']\n"
     ]
    }
   ],
   "source": [
    "encoded_captions_train = [train_vocab.encode_sentence(x) for x in train_captions]\n",
    "\n",
    "print(encoded_captions_train[:1])\n",
    "print(train_vocab.decode_sentence(encoded_captions_train[0]))\n",
    "print(train_captions[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249454"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_captions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from MyDataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249454\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "wat = [torch.tensor(x[1:], dtype=torch.int16) for x in train_vocab.encoded_captions]\n",
    "# wat = [torch.tensor(x, dtype=torch.int16) for x in train_vocab.encoded_captions]\n",
    "padded = pad_sequence(wat).permute(1, 0)\n",
    "print(len(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 35, 9, 4, 397, 31, 5, 140, 2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_captions_train[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 35, 9, 4, 397, 31, 5, 140, 2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vocab.encoded_captions[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([249454, 50])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  4, 145, 430,  10, 667, 450, 237,  35,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(enc_captions=padded,\n",
    "                    image_paths=train_image_paths,\n",
    "                   data_dir=extracted_data_train_dir + 'vecs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small dataste\n",
    "dataset = MyDataset(enc_captions=padded[:25600 * 2],\n",
    "                    image_paths=train_image_paths[:25600 * 2],\n",
    "                   data_dir=extracted_data_train_dir + 'vecs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2048])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=128, \n",
    "                         num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([128, 64, 2048])\n",
      "torch.Size([50])\n",
      "[4, 145, 430, 10, 667, 450, 237, 35, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "a very clean and well decorated empty bathroom <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(dataloader):\n",
    "    imgs, labels = data[0], data[1]\n",
    "    print(idx, imgs.shape)\n",
    "    print(labels[0].shape)\n",
    "    print(labels[0].tolist())\n",
    "    print(train_vocab.decode_sentence(labels[0].tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.Encoder import Encoder\n",
    "from layers.Decoder import Decoder\n",
    "from layers.Attention import Attention\n",
    "from layers.End2End import End2End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEC_EMB_DIM = 512\n",
    "ENC_INPUT = 2048\n",
    "ENC_OUTPUT = 256 \n",
    "DEC_HID_DIM = 512\n",
    "DEC_OUTPUT = 512\n",
    "ATTN_DIM = 512\n",
    "EMB_DIM = 256\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,128,201 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = End2End(ENC_INPUT, ENC_OUTPUT, DEC_HID_DIM, DEC_OUTPUT,\n",
    "               EMB_DIM, ATTN_DIM, train_vocab, criterion, device)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.00025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "\n",
    "def train_step(batch, captions):\n",
    "    \n",
    "    batch_size = captions.shape[0]\n",
    "    caption_length = captions.shape[1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    out, loss = model(batch, captions)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss = loss / int(caption_length)\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.2467\n",
      "Epoch 1 Loss 2.153352\n",
      "Time taken for 1 epoch 307.0429599285126 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.7047\n",
      "Epoch 2 Loss 1.875709\n",
      "Time taken for 1 epoch 278.64416432380676 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.6170\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-ede51f64e06c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Time taken for 1 epoch {} sec\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-ede51f64e06c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mimg_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-1317da46f219>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(batch, captions)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Treca Godina\\ORI\\Image Captioning\\ORI_ML\\layers\\End2End.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, captions)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mmax_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;31m# loss += ((1. - attn_weights.sum(dim=1)) ** 2).mean()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;31m# print(predictions.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 932\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2113\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[0;32m   2114\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "        \n",
    "        steps = 0\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            img_tensor, target, _ = batch[0], batch[1], batch[2]\n",
    "            \n",
    "            batch_loss, t_loss = train_step(img_tensor, target)\n",
    "            total_loss += t_loss.item()\n",
    "            \n",
    "            if idx % 500 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                  epoch + 1, idx, batch_loss.item() / int(target.shape[1])))\n",
    "            steps += 1\n",
    "            \n",
    "        if epoch == 0:\n",
    "            dataloader.dataset.set_use_cache(True)\n",
    "            dataloader.num_workers = 3\n",
    "\n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(total_loss / steps)\n",
    "        \n",
    "        if epoch % 2 == 0 and epoch > 0:\n",
    "            torch.save(model.state_dict(), 'saved_models/model')\n",
    "\n",
    "        print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                             total_loss/steps))\n",
    "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'saved_models/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('saved_models/model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, cap, name = dataset[220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   4,  109,    8,    5,   48,    9,    4, 1675,   10,    4,  170,  803,\n",
       "          23,  104,  530,    6,   37,    2,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0], dtype=torch.int16)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extracted_data/train/vecs/COCO_train2014_000000187042'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = im.unsqueeze(0)\n",
    "cap = cap.unsqueeze(0)\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 2048])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a boat in the water with a raft and a bike loaded at one end of it <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vocab.decode_sentence(cap.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'raft', 'down', 'the', 'background', '<end>']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "out = model.evaluate(im)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess_image_web('https://scontent.fbeg1-1.fna.fbcdn.net/v/t1.15752-9/106495264_270801397340750_5352596102031662927_n.jpg?_nc_cat=111&_nc_sid=b96e70&_nc_eui2=AeHUk7NrGbTNGQAAWPZuOseGdyXefbU8isZ3Jd59tTyKxlrvw8eGUmMqySImBZ4oCy2B2dfMEUR0-VKmJ7nQm1EF&_nc_ohc=f6VChx9r9cEAX_nfj3j&_nc_ht=scontent.fbeg1-1.fna&oh=7f53e6371c6af62fed5640d62a8df943&oe=5F225805')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess_image_web('https://cdn.vox-cdn.com/thumbor/vQ_TbE_e04tsWLhQqMCG40Nn33o=/0x0:600x450/1200x900/filters:focal(0x0:600x450):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/19650209/Kitchen_0719-Brookline_Shot7A-Kitchen_StraightOnFromDiningArea-27.0.0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = preprocess_image('mscoco/train/img/COCO_train2014_000000028149')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape(1,64,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'kitchen', '<end>']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.evaluate(img)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
