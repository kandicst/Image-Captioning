{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data_train_dir = 'extracted_data/train/'\n",
    "extracted_data_test_dir = 'extracted_data/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file):\n",
    "    with open(file, 'r') as f1:\n",
    "        return json.loads(f1.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = load_json(extracted_data_train_dir + 'captions.json')\n",
    "train_image_paths = load_json(extracted_data_train_dir + 'image_paths.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> A bathroom with a TV near the mirror <END>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mscoco/train/img/COCO_train2014_000000028149'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_paths[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_captions = [x[8:] for x in train_captions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mscoco/train/img/COCO_train2014_000000318556 <START> A very clean and well decorated empty bathroom <END>\n"
     ]
    }
   ],
   "source": [
    "print(train_image_paths[0], train_captions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "word_to_idx = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, '<unk>': 3}\n",
    "idx_to_word = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: '<unk>'}\n",
    "\n",
    "unk_idx = word_to_idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_caption(caption):\n",
    "    return caption.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249454"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for caption in train_captions:\n",
    "    caption = preprocess_caption(caption)\n",
    "    for word in caption:\n",
    "        if word in word_freq: word_freq[word] += 1\n",
    "        else: word_freq[word] = 1\n",
    "\n",
    "word_freq.pop(\"<start>\", None)\n",
    "word_freq.pop(\"<end>\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = len(word_to_idx)\n",
    "print(start_idx)\n",
    "\n",
    "# get top K words\n",
    "for word in sorted(word_freq, key=word_freq.get, reverse=True):\n",
    "    word_to_idx[word] = start_idx\n",
    "    idx_to_word[start_idx] = word\n",
    "    start_idx += 1\n",
    "    if start_idx >= max_vocab_size: break\n",
    "\n",
    "len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence):\n",
    "    return [word_to_idx.get(x, unk_idx) for x in preprocess_caption(sentence)]\n",
    "\n",
    "def decode_sentence(indices):\n",
    "    return \" \".join([idx_to_word.get(x, '<unk>') for x in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132, 11, 665, 1931]\n",
      "this is not fun\n"
     ]
    }
   ],
   "source": [
    "sentence = 'This is not fun'\n",
    "encoded = encode_sentence(sentence)\n",
    "decoded = decode_sentence(encoded)\n",
    "\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = Vocabulary(train_captions, max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 125, 473, 10, 742, 480, 230, 38, 2]\n",
      "<start> a very clean and well decorated empty bathroom <end>\n",
      "<START> A very clean and well decorated empty bathroom <END>\n"
     ]
    }
   ],
   "source": [
    "print(train_vocab.encoded_captions[0])\n",
    "print(train_vocab.decode_sentence(train_vocab.encoded_captions[0]))\n",
    "print(train_captions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create encoded captions for each caption in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 125, 473, 10, 742, 480, 230, 38, 2]]\n",
      "<start> a very clean and well decorated empty bathroom <end>\n",
      "['<START> A very clean and well decorated empty bathroom <END>']\n"
     ]
    }
   ],
   "source": [
    "encoded_captions_train = [encode_sentence(x) for x in train_captions]\n",
    "\n",
    "print(encoded_captions_train[:1])\n",
    "print(decode_sentence(encoded_captions_train[0]))\n",
    "print(train_captions[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249454"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_captions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> a bathroom with a tv near the mirror <end>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(encoded_captions_train[55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from MyDataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249454\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "wat = [torch.tensor(x[1:], dtype=torch.int16) for x in train_vocab.encoded_captions]\n",
    "padded = pad_sequence(wat).permute(1, 0)\n",
    "print(len(padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 38, 9, 4, 510, 31, 5, 195, 2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_captions_train[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 38, 9, 4, 510, 31, 5, 195, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vocab.encoded_captions[55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([249454, 50])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  4, 125, 473,  10, 742, 480, 230,  38,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0], dtype=torch.int16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(enc_captions=padded,\n",
    "                    image_paths=train_image_paths,\n",
    "                   data_dir=extracted_data_train_dir + 'vecs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small dataste\n",
    "dataset = MyDataset(enc_captions=padded[:32768],\n",
    "                    image_paths=train_image_paths[:32768],\n",
    "                   data_dir=extracted_data_train_dir + 'vecs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2048])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=dataset, batch_size=256, \n",
    "                         num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([256, 64, 2048])\n",
      "torch.Size([50])\n",
      "[4, 125, 473, 10, 742, 480, 230, 38, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "a very clean and well decorated empty bathroom <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(dataloader):\n",
    "    imgs, labels = data[0], data[1]\n",
    "    print(idx, imgs.shape)\n",
    "    print(labels[0].shape)\n",
    "    print(labels[0].tolist())\n",
    "    print(decode_sentence(labels[0].tolist()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.Encoder import Encoder\n",
    "from layers.Decoder import Decoder\n",
    "from layers.Attention import Attention\n",
    "from layers.End2End import End2End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIM = len(SRC.vocab)\n",
    "# OUTPUT_DIM = len(TRG.vocab)\n",
    "DEC_EMB_DIM = 1024\n",
    "ENC_INPUT = 2048\n",
    "ENC_OUTPUT = 256 #256\n",
    "DEC_HID_DIM = 512\n",
    "DEC_OUTPUT = 512\n",
    "ATTN_DIM = 512\n",
    "EMB_DIM = 256\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,602,889 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = End2End(ENC_INPUT, ENC_OUTPUT, DEC_HID_DIM, DEC_OUTPUT,\n",
    "               EMB_DIM, ATTN_DIM, train_vocab, criterion, device)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "\n",
    "def train_step(batch, captions):\n",
    "    \n",
    "    batch_size = captions.shape[0]\n",
    "    caption_length = captions.shape[1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    _, loss = model(batch, captions)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss = (loss / int(caption_length))\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.1302\n",
      "Epoch 1 Loss 1.991603\n",
      "Time taken for 1 epoch 84.03380751609802 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.2303\n",
      "Epoch 2 Loss 1.634886\n",
      "Time taken for 1 epoch 85.20042419433594 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.2584\n",
      "Epoch 3 Loss 1.545002\n",
      "Time taken for 1 epoch 80.62184619903564 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.2436\n",
      "Epoch 4 Loss 1.496869\n",
      "Time taken for 1 epoch 79.50435876846313 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.2115\n",
      "Epoch 5 Loss 1.470601\n",
      "Time taken for 1 epoch 83.57751083374023 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.2012\n",
      "Epoch 6 Loss 1.399813\n",
      "Time taken for 1 epoch 81.21754169464111 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.1672\n",
      "Epoch 7 Loss 1.349388\n",
      "Time taken for 1 epoch 80.54397511482239 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.1962\n",
      "Epoch 8 Loss 1.325614\n",
      "Time taken for 1 epoch 83.10035920143127 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.1577\n",
      "Epoch 9 Loss 1.277257\n",
      "Time taken for 1 epoch 90.38400435447693 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.1414\n",
      "Epoch 10 Loss 1.251700\n",
      "Time taken for 1 epoch 81.55211210250854 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.1455\n",
      "Epoch 11 Loss 1.220810\n",
      "Time taken for 1 epoch 78.78801417350769 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.1145\n",
      "Epoch 12 Loss 1.188601\n",
      "Time taken for 1 epoch 78.04354763031006 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.1045\n",
      "Epoch 13 Loss 1.166875\n",
      "Time taken for 1 epoch 77.78872966766357 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.0697\n",
      "Epoch 14 Loss 1.159311\n",
      "Time taken for 1 epoch 78.58501505851746 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.1023\n",
      "Epoch 15 Loss 1.152063\n",
      "Time taken for 1 epoch 77.56766414642334 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.0540\n",
      "Epoch 16 Loss 1.131592\n",
      "Time taken for 1 epoch 77.36610174179077 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.0407\n",
      "Epoch 17 Loss 1.121067\n",
      "Time taken for 1 epoch 76.45723795890808 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.0090\n",
      "Epoch 18 Loss 1.108950\n",
      "Time taken for 1 epoch 76.31079339981079 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.0161\n",
      "Epoch 19 Loss 1.093517\n",
      "Time taken for 1 epoch 78.72011494636536 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.9976\n",
      "Epoch 20 Loss 1.073760\n",
      "Time taken for 1 epoch 80.84152293205261 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.9846\n",
      "Epoch 21 Loss 1.058101\n",
      "Time taken for 1 epoch 77.67727327346802 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.9679\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-9f33dbba860b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Time taken for 1 epoch {} sec\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-61-9f33dbba860b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mimg_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0mbatch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#             return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-1733b6f8379a>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(batch, captions)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcaption_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mgrad_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "        \n",
    "        steps = 0\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            img_tensor, target, _ = batch[0], batch[1], batch[2]\n",
    "            \n",
    "            batch_loss, t_loss = train_step(img_tensor, target)\n",
    "            total_loss += t_loss.item()\n",
    "#             return\n",
    "            \n",
    "            if idx % 500 == 0:\n",
    "                print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "                  epoch + 1, idx, batch_loss.item() / int(target.shape[1])))\n",
    "            steps += 1\n",
    "            \n",
    "        if epoch == 0:\n",
    "            dataloader.dataset.set_use_cache(True)\n",
    "            dataloader.num_workers = 3\n",
    "\n",
    "        # storing the epoch end loss value to plot later\n",
    "        loss_plot.append(total_loss / steps)\n",
    "\n",
    "        print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                             total_loss/steps))\n",
    "        print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "im, cap, name = dataset[110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  70,  336,  314,    9,  120, 3319,    7,    4,  355,    2,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0], dtype=torch.int16)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'extracted_data/train/vecs/COCO_train2014_000000568117'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = im.unsqueeze(0)\n",
    "cap = cap.unsqueeze(0)\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 2048])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'three cats sleeping with their owners on a bed. <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(cap.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'woman', 'is', 'standing', 'in', 'the', '<unk>', '<end>']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "out = model.evaluate(im)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'people', '<end>']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.evaluate(im, cap, True)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from data_preprocessing import *\n",
    "import urllib.request\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess_image_web('https://scontent.fbeg1-1.fna.fbcdn.net/v/t1.15752-9/106495264_270801397340750_5352596102031662927_n.jpg?_nc_cat=111&_nc_sid=b96e70&_nc_eui2=AeHUk7NrGbTNGQAAWPZuOseGdyXefbU8isZ3Jd59tTyKxlrvw8eGUmMqySImBZ4oCy2B2dfMEUR0-VKmJ7nQm1EF&_nc_ohc=f6VChx9r9cEAX_nfj3j&_nc_ht=scontent.fbeg1-1.fna&oh=7f53e6371c6af62fed5640d62a8df943&oe=5F225805')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape(1,64,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'is',\n",
       " 'standing',\n",
       " 'in']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.evaluate(img)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
